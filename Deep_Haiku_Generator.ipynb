{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robgon-art/DeepHaiku/blob/main/Deep_Haiku_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep Haiku Generator**\n",
        "## Generating rhythmic prose after finetuning a large transformer with phonemes\n",
        "\n",
        "By Robert. A Gonsalves</br>\n",
        "\n",
        "![image](https://raw.githubusercontent.com/robgon-art/DeepHaiku/main/deep_haiku.jpg)\n",
        "\n",
        "You can see my article on Medium.\n",
        "\n",
        "The source code and generated images are released under the [CC BY-SA license](https://creativecommons.org/licenses/by-sa/4.0/).</br>\n",
        "![CC BYC-SA](https://licensebuttons.net/l/by-sa/3.0/88x31.png)\n",
        "\n",
        "## Acknowledgements\n",
        "- M. Grootendorst, KeyBERT: Minimal keyword extraction with BERT (2020)\n",
        "- W. Zhu and S. Bhat, GRUEN for Evaluating Linguistic Quality of Generated Text (2020)\n",
        "- M. Bernard, Phonemizer: Text to Phones Transcription for Multiple Languages in Python (2016)\n",
        "- GPT-J, Mesh-Transformer-JAX: Model-Parallel Implementation of Transformer Language Model with JAX (2021)\n",
        "- R. Caruana, Multitask learning (1997)\n",
        "- E. Hu, et al., LoRA: Low-rank Adaptation of Large Language Models (2021)\n",
        "- L. Hanu and the Unitary Team, Detoxify (2020)\n",
        "- Trained on Haikus collected by [bfbarry](https://www.kaggle.com/bfbarry/haiku-dataset) and [Harshit Jhalani](https://www.kaggle.com/hjhalani30/haiku-dataset) on Kaggle.com"
      ],
      "metadata": {
        "id": "zaqxwu9moLcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialize the System\n",
        "\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!git clone https://github.com/unitaryai/detoxify\n",
        "!pip install transformers==4.16.2\n",
        "!pip install bitsandbytes-cuda111\n",
        "!git clone https://github.com/robgon-art/GRUEN\n",
        "!pip install wmd\n",
        "!pip install --upgrade --no-cache-dir gdown\n",
        "!gdown --id 1S-l0L_YOzn5KhYHdB8iS37qKwuUhHP0G\n",
        "!gdown --id 10LpkO5Vm_zOu723FVk6cCeRsv_qyYLdL\n",
        "!unzip cola_model.zip\n",
        "!pip install phonemizer\n",
        "!sudo apt-get install festival\n",
        "\n",
        "import transformers\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.cuda.amp import custom_fwd, custom_bwd\n",
        "from bitsandbytes.functional import quantize_blockwise, dequantize_blockwise\n",
        "from tqdm.auto import tqdm\n",
        "from phonemizer import phonemize\n",
        "from phonemizer.separator import Separator\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import sys\n",
        "sys.path.append(\"GRUEN\")\n",
        "import GRUEN.Main as gruen\n",
        "import sys\n",
        "sys.path.append(\"/content/detoxify\")\n",
        "from detoxify import Detoxify\n",
        "\n",
        "def get_festival_phonemes(line):\n",
        "  phn = phonemize(line, language='en-us', backend='festival', with_stress=False,\n",
        "      separator=Separator(phone=None, word=' ', syllable=\"|\"), strip=True)\n",
        "  return phn\n",
        "\n",
        "text = [\"pet pug arthur\"]\n",
        "print(get_festival_phonemes(text))\n",
        "\n",
        "def count_syllables(doc):\n",
        "  phrases = doc.split(\"/\")\n",
        "  counts = []\n",
        "  for p in phrases:\n",
        "    count = 0\n",
        "    words = p.split(\" \")\n",
        "    for w in words:\n",
        "      syllables = w.split(\"|\")\n",
        "      count += len(syllables)\n",
        "    counts.append(count)\n",
        "  return counts\n",
        "\n",
        "print(count_syllables(\"waa|shihng dhax dih|shaxz/thihng|kaxng ax|bawt ax hhay|kuw/werdz flow layk wao|ter\"))\n",
        "\n",
        "doc = [\"Autumn is nearing. Golden trees tempting my eyes. While songs tease my ears\"]\n",
        "gruen_results = gruen.get_gruen(doc)\n",
        "print(\"gruen: \", gruen_results)\n",
        "\n",
        "from detoxify import Detoxify\n",
        "results = Detoxify('original').predict(\"Autumn is nearing / Golden trees tempting my eyes / While songs tease my ears\")\n",
        "for key, value in results.items():\n",
        "  print(key, ' : ', round(value, 5))\n",
        "\n",
        "class FrozenBNBLinear(nn.Module):\n",
        "    def __init__(self, weight, absmax, code, bias=None):\n",
        "        assert isinstance(bias, nn.Parameter) or bias is None\n",
        "        super().__init__()\n",
        "        self.out_features, self.in_features = weight.shape\n",
        "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
        "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
        "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
        "        self.adapter = None\n",
        "        self.bias = bias\n",
        " \n",
        "    def forward(self, input):\n",
        "        output = DequantizeAndLinear.apply(input, self.weight, self.absmax, self.code, self.bias)\n",
        "        if self.adapter:\n",
        "            output += self.adapter(input)\n",
        "        return output\n",
        " \n",
        "    @classmethod\n",
        "    def from_linear(cls, linear: nn.Linear) -> \"FrozenBNBLinear\":\n",
        "        weights_int8, state = quantize_blockise_lowmemory(linear.weight)\n",
        "        return cls(weights_int8, *state, linear.bias)\n",
        " \n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}({self.in_features}, {self.out_features})\"\n",
        " \n",
        " \n",
        "class DequantizeAndLinear(torch.autograd.Function): \n",
        "    @staticmethod\n",
        "    @custom_fwd\n",
        "    def forward(ctx, input: torch.Tensor, weights_quantized: torch.ByteTensor,\n",
        "                absmax: torch.FloatTensor, code: torch.FloatTensor, bias: torch.FloatTensor):\n",
        "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
        "        ctx.save_for_backward(input, weights_quantized, absmax, code)\n",
        "        ctx._has_bias = bias is not None\n",
        "        return F.linear(input, weights_deq, bias)\n",
        " \n",
        "    @staticmethod\n",
        "    @custom_bwd\n",
        "    def backward(ctx, grad_output: torch.Tensor):\n",
        "        assert not ctx.needs_input_grad[1] and not ctx.needs_input_grad[2] and not ctx.needs_input_grad[3]\n",
        "        input, weights_quantized, absmax, code = ctx.saved_tensors\n",
        "        # grad_output: [*batch, out_features]\n",
        "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
        "        grad_input = grad_output @ weights_deq\n",
        "        grad_bias = grad_output.flatten(0, -2).sum(dim=0) if ctx._has_bias else None\n",
        "        return grad_input, None, None, None, grad_bias\n",
        " \n",
        " \n",
        "class FrozenBNBEmbedding(nn.Module):\n",
        "    def __init__(self, weight, absmax, code):\n",
        "        super().__init__()\n",
        "        self.num_embeddings, self.embedding_dim = weight.shape\n",
        "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
        "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
        "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
        "        self.adapter = None\n",
        " \n",
        "    def forward(self, input, **kwargs):\n",
        "        with torch.no_grad():\n",
        "            # note: both quantuized weights and input indices are *not* differentiable\n",
        "            weight_deq = dequantize_blockwise(self.weight, absmax=self.absmax, code=self.code)\n",
        "            output = F.embedding(input, weight_deq, **kwargs)\n",
        "        if self.adapter:\n",
        "            output += self.adapter(input)\n",
        "        return output \n",
        " \n",
        "    @classmethod\n",
        "    def from_embedding(cls, embedding: nn.Embedding) -> \"FrozenBNBEmbedding\":\n",
        "        weights_int8, state = quantize_blockise_lowmemory(embedding.weight)\n",
        "        return cls(weights_int8, *state)\n",
        " \n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}({self.num_embeddings}, {self.embedding_dim})\"\n",
        " \n",
        "def quantize_blockise_lowmemory(matrix: torch.Tensor, chunk_size: int = 2 ** 20):\n",
        "    assert chunk_size % 4096 == 0\n",
        "    code = None\n",
        "    chunks = []\n",
        "    absmaxes = []\n",
        "    flat_tensor = matrix.view(-1)\n",
        "    for i in range((matrix.numel() - 1) // chunk_size + 1):\n",
        "        input_chunk = flat_tensor[i * chunk_size: (i + 1) * chunk_size].clone()\n",
        "        quantized_chunk, (absmax_chunk, code) = quantize_blockwise(input_chunk, code=code)\n",
        "        chunks.append(quantized_chunk)\n",
        "        absmaxes.append(absmax_chunk)\n",
        " \n",
        "    matrix_i8 = torch.cat(chunks).reshape_as(matrix)\n",
        "    absmax = torch.cat(absmaxes)\n",
        "    return matrix_i8, (absmax, code)\n",
        " \n",
        " \n",
        "def convert_to_int8(model):\n",
        "    \"\"\"Convert linear and embedding modules to 8-bit with optional adapters\"\"\"\n",
        "    for module in list(model.modules()):\n",
        "        for name, child in module.named_children():\n",
        "            if isinstance(child, nn.Linear):\n",
        "                print(name, child)\n",
        "                setattr( \n",
        "                    module,\n",
        "                    name,\n",
        "                    FrozenBNBLinear(\n",
        "                        weight=torch.zeros(child.out_features, child.in_features, dtype=torch.uint8),\n",
        "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
        "                        code=torch.zeros(256),\n",
        "                        bias=child.bias,\n",
        "                    ),\n",
        "                )\n",
        "            elif isinstance(child, nn.Embedding):\n",
        "                setattr(\n",
        "                    module,\n",
        "                    name,\n",
        "                    FrozenBNBEmbedding(\n",
        "                        weight=torch.zeros(child.num_embeddings, child.embedding_dim, dtype=torch.uint8),\n",
        "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
        "                        code=torch.zeros(256),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "class GPTJBlock(transformers.models.gptj.modeling_gptj.GPTJBlock):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        convert_to_int8(self.attn)\n",
        "        convert_to_int8(self.mlp)\n",
        "\n",
        "\n",
        "class GPTJModel(transformers.models.gptj.modeling_gptj.GPTJModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        convert_to_int8(self)\n",
        "        \n",
        "\n",
        "class GPTJForCausalLM(transformers.models.gptj.modeling_gptj.GPTJForCausalLM):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        convert_to_int8(self)\n",
        "\n",
        "\n",
        "transformers.models.gptj.modeling_gptj.GPTJBlock = GPTJBlock  # monkey-patch GPT-J\n",
        "\n",
        "import transformers\n",
        "config = transformers.GPTJConfig.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
        "\n",
        "gpt = torch.load(\"/content/gpt-j-8bit_deep_haikul.pt\",  map_location=torch.device('cuda'))\n",
        "gpt.eval()"
      ],
      "metadata": {
        "id": "jPU9dV2I6nJh",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate Haikus\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "topic = 'autumn' #@param {type:\"string\"}\n",
        "only_show_5_7_5 = True #@param {type:\"boolean\"}\n",
        "quality_filter = 0.45 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "toxicity_filter = 0.85 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "\n",
        "prompt = \"(\" + topic.strip()\n",
        "if not \"=\" in topic:\n",
        "  prompt += \" =\"\n",
        "# print(\"'\" + prompt + \"'\")\n",
        "with torch.no_grad():\n",
        "  prompt_tokens = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
        "  sample_outputs = gpt.generate(prompt_tokens, max_length=40, do_sample=True, \n",
        "    num_return_sequences=20, temperature=0.8)\n",
        "    \n",
        "haikus = []\n",
        "haikus_no_slashes = []\n",
        "\n",
        "for i, sample_output in enumerate(sample_outputs):\n",
        "  doc = (tokenizer.decode(sample_outputs[i], skip_special_tokens=True))\n",
        "  parts = doc.split(\")\")\n",
        "  haiku_with_prompt = parts[0][1:].strip()\n",
        "  haiku = haiku_with_prompt.split(\" = \")[1].strip()\n",
        "  haikus.append(haiku)\n",
        "  haikus_no_slashes.append(haiku.replace(\" / \", \" \"))\n",
        "\n",
        "quality = gruen.get_gruen(haikus_no_slashes)\n",
        "\n",
        "print()\n",
        "heading = \"Deep Haiku Generation for \" + topic\n",
        "print(heading.ljust(90) + \"Syllables\".ljust(12) + \"Quality\".ljust(10) + \"Toxicity\")\n",
        "\n",
        "import re\n",
        "for h, q in zip(haikus, quality):\n",
        "  hp = []\n",
        "  parts = h.split(\"/\")\n",
        "  for part in parts:\n",
        "    part = part.strip()\n",
        "    part = re.sub(r\"[^A-Za-z| ]+\", \"\", part)\n",
        "    p = get_festival_phonemes(part)\n",
        "    hp.append(p)\n",
        "  phones = \"/\".join(hp)\n",
        "  syllables = count_syllables(phones)\n",
        "  if not only_show_5_7_5 or syllables == [5, 7, 5]:\n",
        "    results = Detoxify('original').predict(h)\n",
        "    t = results[\"toxicity\"]\n",
        "    # print(h, syllables, s, t)\n",
        "    # if True:\n",
        "    if q > quality_filter and t < toxicity_filter: \n",
        "      h = h.replace(\". / \", \" / \")\n",
        "      if h.endswith(\".\"):\n",
        "        h = h[:-1]\n",
        "      print(h.ljust(90) + str(syllables).ljust(12) + str(round(q, 5)).ljust(10) + \n",
        "            str(round(t,5)))\n",
        "      # print(h)"
      ],
      "metadata": {
        "id": "QCzJo_1N6bwN",
        "cellView": "form",
        "outputId": "7f062930-091e-4b18-fe2e-673781053dbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 20/20 [00:00<00:00, 26.03it/s]\n",
            "Evaluating: 100%|██████████| 59/59 [00:00<00:00, 65.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Deep Haiku Generation for autumn                                                          Syllables   Quality   Toxicity\n",
            "Autumn arrives soon / Without warning once again / I stare at the trees                   [5, 7, 5]   0.75344   0.00078\n",
            "Autumn air fills the / House with salty, sweet candy / Apples, cinnamon                   [5, 7, 5]   0.56015   0.00062\n",
            "Autumn winds arrive / Parading through the tree tops / Leaf colors flutter                [5, 7, 5]   0.78927   0.0006\n",
            "Autumn air, cool breeze / Candy colors painted skies / Fall is near, but come             [5, 7, 5]   0.69261   0.00062\n",
            "Autumn winds arrive / Parading through the walnut / Panic in the hills                    [5, 7, 5]   0.6862    0.00073\n",
            "Autumn Equinox / Where the sun and moon are in? /balance this night is                    [5, 7, 5]   0.60462   0.00073\n",
            "I think it's okay / To eat a little bit of / Candy in autumn                              [5, 7, 5]   0.57373   0.00067\n",
            "Autumn colours here / Rainy days are beautiful / Holiday heaven                           [5, 7, 5]   0.73633   0.00058\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Deep Haiku Generator",
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}